{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "600c248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32da2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ccd92eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacmanEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, grid_size=[50, 50]):\n",
    "        super(PacmanEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        \n",
    "        # [agent_x, agent_y, pellet_x, pellet_y, prev_agent_action] + \n",
    "        #     [left_wall_dist, right_wall_dist, top_wall_dist, bottom_wall_dist]\n",
    "        self.observation_space = spaces.MultiDiscrete([\n",
    "                self.grid_size[0], # agent_x\n",
    "                self.grid_size[1], # agent_y\n",
    "                self.grid_size[0], # pellet_x\n",
    "                self.grid_size[1], # pellet_y\n",
    "                4, #  prev_agent_action\n",
    "                self.grid_size[0], # left_wall_dist\n",
    "                self.grid_size[0], # right_wall_dist\n",
    "                self.grid_size[1], # top_wall_dist\n",
    "                self.grid_size[1], # bottom_wall_dist\n",
    "            ])\n",
    "\n",
    "        # We have 4 actions, corresponding to \"left\", \"up\", \"right\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.grid = np.zeros(self.grid_size)\n",
    "        # set wall/obstacle to 1\n",
    "        self.grid[0, :] = 1\n",
    "        self.grid[-1, :] = 1\n",
    "        self.grid[:, -1] = 1\n",
    "        self.grid[:, 0] = 1\n",
    "        # the position of pacman and pellet will not be mapped on the grid.\n",
    "        # instead, we will keep track of it independently\n",
    "        # initiate pacman and pellet position\n",
    "        self.pacman_position = [5, 5] #[np.random.randint(self.grid_size[0]), np.random.randint(self.grid_size[1])]\n",
    "        self.pellet_position = [np.random.randint(1, self.grid_size[0]-1), np.random.randint(1, self.grid_size[1]-1)]\n",
    "        self.score = 0\n",
    "        self.prev_reward = 0\n",
    "        self.action = 0\n",
    "        \n",
    "        self.done = False\n",
    "\n",
    "        agent_x = self.pacman_position[0]\n",
    "        agent_y = self.pacman_position[1]\n",
    "        pellet_x = self.pellet_position[0]\n",
    "        pellet_y = self.pellet_position[1]\n",
    "        self.prev_action = 0\n",
    "        left_wall_dist = agent_x - 1\n",
    "        right_wall_dist = (self.grid_size[0]-2) - agent_x\n",
    "        top_wall_dist = agent_y - 1\n",
    "        bottom_wall_dist = (self.grid_size[1]-2) - agent_y\n",
    "\n",
    "        # create observation:\n",
    "        observation = [agent_x, agent_y, pellet_x, pellet_y, self.prev_action, \n",
    "                           left_wall_dist, right_wall_dist, top_wall_dist, bottom_wall_dist]\n",
    "        observation = np.array(observation)\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.pacman_position[0] -= 1\n",
    "        elif action == 1:\n",
    "            self.pacman_position[1] -= 1\n",
    "        elif action == 2:\n",
    "            self.pacman_position[0] += 1\n",
    "        elif action == 3:\n",
    "            self.pacman_position[1] += 1 \n",
    "        \n",
    "        # On collision kill the snake and print the score\n",
    "        if self.grid[self.pacman_position[0], self.pacman_position[1]] == 1:\n",
    "#             print('hit a wall')\n",
    "            self.done = True\n",
    "            self.reward = -10\n",
    "        \n",
    "        # reward is based on the distance between pacman and pellet\n",
    "        grid_hypothenus = sqrt(self.grid_size[0]**2 + self.grid_size[1]**2)\n",
    "        pacman_pellet_distance = sqrt(abs(self.pacman_position[0] - self.pellet_position[0])**2 + (self.pacman_position[1] - self.pellet_position[1])**2)\n",
    "        self.total_reward = 1 - (pacman_pellet_distance/grid_hypothenus)\n",
    "        self.reward = self.total_reward - self.prev_reward\n",
    "        self.prev_reward = self.total_reward\n",
    "            \n",
    "        if self.pacman_position == self.pellet_position:\n",
    "#             print('found pellet')\n",
    "            self.reward = 5\n",
    "            self.done = True\n",
    "\n",
    "        agent_x = self.pacman_position[0]\n",
    "        agent_y = self.pacman_position[1]\n",
    "        pellet_x = self.pellet_position[0]\n",
    "        pellet_y = self.pellet_position[1]\n",
    "        self.prev_actions = action\n",
    "        left_wall_dist = agent_x - 1\n",
    "        right_wall_dist = (self.grid_size[0]-2) - agent_x\n",
    "        top_wall_dist = agent_y - 1\n",
    "        bottom_wall_dist = (self.grid_size[1]-2) - agent_y\n",
    "\n",
    "        observation = [agent_x, agent_y, pellet_x, pellet_y, self.prev_action, \n",
    "                           left_wall_dist, right_wall_dist, top_wall_dist, bottom_wall_dist]\n",
    "        observation = np.array(observation)\n",
    "        \n",
    "        info = {}\n",
    "        return observation, self.reward, self.done, info\n",
    "    \n",
    "    def render(self, step_count):\n",
    "        game_visual = np.zeros((self.grid_size[0], self.grid_size[1], 3), np.uint8)\n",
    "        wall_pos = np.where(self.grid==1)\n",
    "        for index, _ in enumerate(wall_pos[0]):\n",
    "            game_visual[wall_pos[0][index], wall_pos[1][index]] = (89, 28, 17) \n",
    "        \n",
    "        if self.pacman_position == self.pellet_position:\n",
    "            game_visual[self.pellet_position[0], self.pellet_position[1]] = (0, 255, 0) \n",
    "#             game_visual = cv2.putText(game_visual, f'1111', (-5, 6), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "#                    1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            game_visual[self.pacman_position[0], self.pacman_position[1]] = (255, 255, 255) \n",
    "            game_visual[self.pellet_position[0], self.pellet_position[1]] = (0, 0, 255) \n",
    "            \n",
    "        game_visual = game_visual.repeat(10, axis=0).repeat(2, axis=1)\n",
    "        cv2.namedWindow(\"Pacman Simplified\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Pacman Simplified\", 500, 500)\n",
    "        cv2.imshow('Pacman Simplified', game_visual)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "        print('done')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dd8c9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PacmanEnv(grid_size=[10, 10])\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6249946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | 1.83     |\n",
      "| time/              |          |\n",
      "|    fps             | 1527     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 21.7        |\n",
      "|    ep_rew_mean          | 1.92        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1074        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008365192 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0981     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.2        |\n",
      "|    ep_rew_mean          | 2.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 980         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013318558 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.599       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 1.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.8        |\n",
      "|    ep_rew_mean          | 2.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 939         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015229929 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.381       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | 3.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 916         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015687475 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.0167      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.522       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.4        |\n",
      "|    ep_rew_mean          | 4.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 902         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014376327 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.836       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 17.9       |\n",
      "|    ep_rew_mean          | 5.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 891        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01641753 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | 0.253      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.861      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 1.42       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.6       |\n",
      "|    ep_rew_mean          | 5.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01906646 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.347      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.75       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    value_loss           | 1.15       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.1        |\n",
      "|    ep_rew_mean          | 5.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017092919 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.457       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.3        |\n",
      "|    ep_rew_mean          | 5.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017689016 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.57       |\n",
      "|    ep_rew_mean          | 5.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 867        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01855303 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.659      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    value_loss           | 0.881      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.94        |\n",
      "|    ep_rew_mean          | 5.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 864         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018874638 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0866      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.62        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.89       |\n",
      "|    ep_rew_mean          | 5.88       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 862        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01438292 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | -0.0863    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.291      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    value_loss           | 0.429      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.24        |\n",
      "|    ep_rew_mean          | 5.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016148653 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0069      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.95        |\n",
      "|    ep_rew_mean          | 5.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016512936 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.847      |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.263       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | 5.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019052636 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.773      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.21        |\n",
      "|    ep_rew_mean          | 5.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014383338 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.217       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.14       |\n",
      "|    ep_rew_mean          | 5.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 853        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01571067 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.682     |\n",
      "|    explained_variance   | 0.228      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00118   |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.51        |\n",
      "|    ep_rew_mean          | 5.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015640968 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.334       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.334       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.2         |\n",
      "|    ep_rew_mean          | 5.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014300742 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0877      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f4385b6dc0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log='logs')\n",
    "model.learn(total_timesteps=40000, tb_log_name='PPO')\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b2f2459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "4\n",
      "10\n",
      "3\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         step_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step_count)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for eps in range(episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    env.render(step_count)\n",
    "    while not done:\n",
    "#         action = env.action_space.sample()\n",
    "        action = model.predict(observation)[0]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render(step_count)\n",
    "        time.sleep(0.1)\n",
    "#         print(f'total reward: {env.total_reward}')\n",
    "        step_count+=1\n",
    "    print(step_count)\n",
    "    time.sleep(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "57ba6290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 0.7298148782778742\n",
      "total reward: 0.72\n",
      "total reward: 0.7298148782778742\n",
      "total reward: 0.72\n",
      "total reward: 0.7298148782778742\n",
      "total reward: 0.719286623047636\n",
      "total reward: 0.7098276374290617\n",
      "total reward: 0.6993340724325419\n",
      "total reward: 0.6898387516145836\n",
      "total reward: 0.7\n",
      "0.010161248385416322\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "episodes = 10\n",
    "for eps in range(episodes):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "    print(f'total reward: {env.total_reward}')\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(env.reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068f7d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.pellet_position == [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27140c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.pacman_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00311225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
